{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline to tune  tf-idf + ridge regularization parameters and select the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to dabble a bit into text mining in this post. The idea is very simple: we have a collection of documents (these could be emails, books or craiglist ads) and we are trying to build a model that predicts something when given a new document of the same provenance. To make this more concrete we will look at two examples: \n",
    "- predicting the salary offer for a job based on the description of the job listing \n",
    "- predicing whether a text message is spam. \n",
    "\n",
    "Along the way I will also explore how to build pipelines in python using sklearn and how to use tf-idf to transform the documents into numeric matrices. I am pretty new to all of this myself (mostly writing this up so I don't forget) so any suggestions and corrections are welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the required python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the salary listing first. We are going to try to build a model that predicts the salary offer for a job based on the description of the job listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/ajschumacher/gadsdata/master/salary/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train.SalaryNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                             Title  \\\n",
       "0  12612628       Engineering Systems Analyst   \n",
       "1  12612830           Stress Engineer Glasgow   \n",
       "2  12612844  Modelling and simulation analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                    SalaryRaw  SalaryNormalized        SourceName  \n",
       "0  20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1  25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2  20000 - 40000/annum 20-40K             30000  cv-library.co.uk  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at what a posting looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stress Engineer Glasgow Salary **** to **** We re currently looking for talented engineers to join our growing Glasgow team at a variety of levels. The roles are ideally suited to high calibre engineering graduates with any level of appropriate experience, so that we can give you the opportunity to use your technical skills to provide high quality input to our aerospace projects, spanning both aerostructures and aeroengines. In return, you can expect good career opportunities and the chance for advancement and personal and professional development, support while you gain Chartership and some opportunities to possibly travel or work in other offices, in or outside of the UK. The Requirements You will need to have a good engineering degree that includes structural analysis (such as aeronautical, mechanical, automotive, civil) with some experience in a professional engineering environment relevant to (but not limited to) the aerospace sector. You will need to demonstrate experience in at least one or more of the following areas: Structural/stress analysis Composite stress analysis (any industry) Linear and nonlinear finite element analysis Fatigue and damage tolerance Structural dynamics Thermal analysis Aerostructures experience You will also be expected to demonstrate the following qualities: A strong desire to progress quickly to a position of leadership Professional approach Strong communication skills, written and verbal Commercial awareness Team working, being comfortable working in international teams and self managing PLEASE NOTE SECURITY CLEARANCE IS REQUIRED FOR THIS ROLE Stress Engineer Glasgow Salary **** to ****'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.FullDescription[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just use the description and build a pipeline to predict the Normalized Salary. This is quite easy in sklearn using a pipeline. Basically we will create a bag of words then scale the columns using tf_idf. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Then we will fit a regularized linear model to the data. Regularization is key here since when using bi-grams we'll end up with over 400k features and only 10k training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = [(\"tf_idf\", TfidfVectorizer()), \n",
    "              (\"ridge\", linear_model.Ridge())]\n",
    "model = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we just plug in the raw descriptions and the tf_idf transforms it into a matrix that is then fitted by the ridge model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### $$Description\\longrightarrow X , y \\longrightarrow model$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf_idf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.FullDescription, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both the tf_idf transform and the ridge regression have tuning parameters and the nice thing about the pipeline we just built is that we can tune all the parameters at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"ridge__alpha\":[0.1, 0.3, 1, 3, 10], #regularization param\n",
    "          \"tf_idf__min_df\": [1, 3, 10], #min count of words allowed\n",
    "          \"tf_idf__ngram_range\": [(1,1), (1,2)], #1-grams or 2-grams\n",
    "          \"tf_idf__stop_words\": [None, \"english\"]} #use stopwords or don't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different model must we run? Well since we're doing a grid search we can just multiply the possibilities for each parameter to get `5*3*2*2` for a total of 60 models - a decent number. And keep in mind that for each model we have to build the tf_idf vectorizer all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid = params, scoring = \"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid.fit(train.FullDescription, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 0.3,\n",
       " 'tf_idf__min_df': 1,\n",
       " 'tf_idf__ngram_range': (1, 2),\n",
       " 'tf_idf__stop_words': 'english'}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10532.473521325306"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(-grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at all the params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = pd.DataFrame([i[0] for i in grid.grid_scores_])\n",
    "results = pd.DataFrame(grid.grid_scores_)\n",
    "results = pd.concat([params, results], 1)\n",
    "results[\"rmse\"] = np.sqrt(-results.mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge__alpha</th>\n",
       "      <th>tf_idf__min_df</th>\n",
       "      <th>tf_idf__ngram_range</th>\n",
       "      <th>tf_idf__stop_words</th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_validation_score</th>\n",
       "      <th>cv_validation_scores</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>-1.383986e+08</td>\n",
       "      <td>[-103831685.851, -141229157.862, -170145315.841]</td>\n",
       "      <td>11764.293270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>-1.408870e+08</td>\n",
       "      <td>[-105929048.004, -144749023.148, -171993435.294]</td>\n",
       "      <td>11869.583228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>-1.113026e+08</td>\n",
       "      <td>[-77620035.3972, -108499379.09, -147798481.11]</td>\n",
       "      <td>10550.004578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ridge__alpha  tf_idf__min_df tf_idf__ngram_range tf_idf__stop_words  \\\n",
       "0           0.1               1              (1, 1)               None   \n",
       "1           0.1               1              (1, 1)            english   \n",
       "2           0.1               1              (1, 2)               None   \n",
       "\n",
       "                                          parameters  mean_validation_score  \\\n",
       "0  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...          -1.383986e+08   \n",
       "1  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...          -1.408870e+08   \n",
       "2  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...          -1.113026e+08   \n",
       "\n",
       "                               cv_validation_scores          rmse  \n",
       "0  [-103831685.851, -141229157.862, -170145315.841]  11764.293270  \n",
       "1  [-105929048.004, -144749023.148, -171993435.294]  11869.583228  \n",
       "2    [-77620035.3972, -108499379.09, -147798481.11]  10550.004578  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time we predict the model will run the tf-idf part first, already fitted on the train set and then use the ridge regression model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25975.84531928,  32824.5058169 ,  32127.26976225, ...,\n",
       "        50386.2916183 ,  50138.40072399,  27588.69246637])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train.FullDescription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with using the pipeline is that we don't see the little details that go into fitting the models.\n",
    "\n",
    "What if we want to examing more closely what goes on in each model? Say for example I want to look at the coefficients of my linear regression. That's also pretty straighforward using the `named_steps` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -465.8824938 ,  1697.39286267,  1304.56896049, ...,  1416.89223231,\n",
       "        -596.29992468,  -596.29992468])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.named_steps[\"ridge\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_model = model.named_steps[\"ridge\"]\n",
    "tf_idf_model = model.named_steps[\"tf_idf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\"names\":tf_idf_model.get_feature_names(),\n",
    "                             \"coef\":ridge_model.coef_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tokens with the largest coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88432</th>\n",
       "      <td>51890.453609</td>\n",
       "      <td>consultant grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235331</th>\n",
       "      <td>48488.999766</td>\n",
       "      <td>locum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399929</th>\n",
       "      <td>45052.453063</td>\n",
       "      <td>subsea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174963</th>\n",
       "      <td>43441.208259</td>\n",
       "      <td>global</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235338</th>\n",
       "      <td>40651.232641</td>\n",
       "      <td>locum consultant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90843</th>\n",
       "      <td>40016.083870</td>\n",
       "      <td>contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235682</th>\n",
       "      <td>38554.092136</td>\n",
       "      <td>london</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211090</th>\n",
       "      <td>36076.259999</td>\n",
       "      <td>investment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121657</th>\n",
       "      <td>34280.854922</td>\n",
       "      <td>director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244094</th>\n",
       "      <td>33309.500667</td>\n",
       "      <td>manager</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef             names\n",
       "88432   51890.453609  consultant grade\n",
       "235331  48488.999766             locum\n",
       "399929  45052.453063            subsea\n",
       "174963  43441.208259            global\n",
       "235338  40651.232641  locum consultant\n",
       "90843   40016.083870          contract\n",
       "235682  38554.092136            london\n",
       "211090  36076.259999        investment\n",
       "121657  34280.854922          director\n",
       "244094  33309.500667           manager"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients.sort_values(\"coef\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some of the usual suspects - such as london, consultant, director and manager. However given how many features we have (over 400k) it's hard to interpret these coefficients very accurately. Perhaps doing a Lasso model with a strong $l_1$ regularization might help with that, since that wil reduce the number of non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam example:\n",
    "Now let's look at another classical text analysis problem - clasifying wether an email (or text message) is spam or not. Let's load up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'\n",
    "sms = pd.read_table(url, header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.iloc[12, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok this one is clearly spam :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corp = sms.message\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(sms.label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this case we are prediting a class - ham vs. spam so linear regression won't cut it. \n",
    "So do we need to go thru the whole process of building the pipeline again? Well not really. The tf-idf part stays the same we just have to use logistic (instead of linear) regression. So we simply replace `linear_model.Ridge()` with `linear_model.RidgeClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = [(\"tf_idf\", TfidfVectorizer()), \n",
    "              (\"ridge\", linear_model.RidgeClassifier())]\n",
    "model = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"ridge__alpha\":[0.1, 0.3, 1, 3, 10], #regularization param\n",
    "          \"tf_idf__min_df\": [1, 3, 10], #min count of words allowed\n",
    "          \"tf_idf__ngram_range\": [(1,1), (1,2)], #1-grams or 2-grams\n",
    "          \"tf_idf__stop_words\": [None, \"english\"],#use stopwords or don't\n",
    "          \"tf_idf__use_idf\":[True, False]}  #whether to scale columns or just leave normalized bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('tf_idf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...True,\n",
       "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "        tol=0.001))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'ridge__alpha': [0.1, 0.3, 1, 3, 10], 'tf_idf__stop_words': [None, 'english'], 'tf_idf__min_df': [1, 3, 10], 'tf_idf__ngram_range': [(1, 1), (1, 2)], 'tf_idf__use_idf': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(corp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 0.1,\n",
       " 'tf_idf__min_df': 1,\n",
       " 'tf_idf__ngram_range': (1, 2),\n",
       " 'tf_idf__stop_words': None,\n",
       " 'tf_idf__use_idf': True}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = pd.DataFrame([i[0] for i in grid.grid_scores_])\n",
    "results = pd.DataFrame(grid.grid_scores_)\n",
    "results = pd.concat([params, results], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ridge__alpha</th>\n",
       "      <th>tf_idf__min_df</th>\n",
       "      <th>tf_idf__ngram_range</th>\n",
       "      <th>tf_idf__stop_words</th>\n",
       "      <th>tf_idf__use_idf</th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_validation_score</th>\n",
       "      <th>cv_validation_scores</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.981874</td>\n",
       "      <td>[0.982238966631, 0.980613893376, 0.982767905223]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.982233</td>\n",
       "      <td>[0.983315392896, 0.981152396338, 0.982229402262]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.980617</td>\n",
       "      <td>[0.980624327234, 0.980075390415, 0.981152396338]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.981694</td>\n",
       "      <td>[0.980624327234, 0.981152396338, 0.983306408185]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.987437</td>\n",
       "      <td>[0.987621097955, 0.985460420032, 0.989229940765]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.986540</td>\n",
       "      <td>[0.985468245425, 0.985998922994, 0.988152934841]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.983668</td>\n",
       "      <td>[0.981162540366, 0.984921917071, 0.984921917071]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.983668</td>\n",
       "      <td>[0.983315392896, 0.983306408185, 0.984383414109]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.982412</td>\n",
       "      <td>[0.983853606028, 0.982229402262, 0.981152396338]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.982950</td>\n",
       "      <td>[0.983315392896, 0.983306408185, 0.982229402262]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.977925</td>\n",
       "      <td>[0.981700753498, 0.975767366721, 0.976305869682]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.978643</td>\n",
       "      <td>[0.981700753498, 0.976844372644, 0.977382875606]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.985463</td>\n",
       "      <td>[0.98439181916, 0.985998922994, 0.985998922994]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.985463</td>\n",
       "      <td>[0.984930032293, 0.984921917071, 0.986537425956]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.980079</td>\n",
       "      <td>[0.981162540366, 0.978998384491, 0.980075390415]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.980797</td>\n",
       "      <td>[0.982777179763, 0.980613893376, 0.978998384491]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.980976</td>\n",
       "      <td>[0.982777179763, 0.978459881529, 0.9816908993]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.980438</td>\n",
       "      <td>[0.981700753498, 0.978998384491, 0.980613893376]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.976849</td>\n",
       "      <td>[0.977395048439, 0.976305869682, 0.976844372644]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.977028</td>\n",
       "      <td>[0.977933261572, 0.975228863759, 0.977921378568]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.982053</td>\n",
       "      <td>[0.983315392896, 0.982229402262, 0.980613893376]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.981515</td>\n",
       "      <td>[0.983853606028, 0.9816908993, 0.978998384491]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.978284</td>\n",
       "      <td>[0.980624327234, 0.977382875606, 0.976844372644]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.978464</td>\n",
       "      <td>[0.979009687836, 0.977382875606, 0.978998384491]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.3, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.983309</td>\n",
       "      <td>[0.98439181916, 0.9816908993, 0.983844911147]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.3, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.982950</td>\n",
       "      <td>[0.98439181916, 0.981152396338, 0.983306408185]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.3, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>[0.983315392896, 0.981152396338, 0.983306408185]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.3, 'tf_idf__stop_words': 'e...</td>\n",
       "      <td>0.982053</td>\n",
       "      <td>[0.983853606028, 0.980613893376, 0.9816908993]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 0.3, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.987258</td>\n",
       "      <td>[0.987621097955, 0.984921917071, 0.989229940765]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 0.3, 'tf_idf__stop_words': No...</td>\n",
       "      <td>0.986181</td>\n",
       "      <td>[0.984930032293, 0.985460420032, 0.988152934841]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...</td>\n",
       "      <td>0.975951</td>\n",
       "      <td>[0.976318622174, 0.973074851912, 0.978459881529]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...</td>\n",
       "      <td>0.975592</td>\n",
       "      <td>[0.975780409042, 0.97253634895, 0.978459881529]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 3, 'tf_idf__stop_words': None...</td>\n",
       "      <td>0.980438</td>\n",
       "      <td>[0.981700753498, 0.980075390415, 0.979536887453]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 3, 'tf_idf__stop_words': None...</td>\n",
       "      <td>0.978643</td>\n",
       "      <td>[0.978471474704, 0.978998384491, 0.978459881529]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...</td>\n",
       "      <td>0.977028</td>\n",
       "      <td>[0.976318622174, 0.974690360797, 0.980075390415]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...</td>\n",
       "      <td>0.975592</td>\n",
       "      <td>[0.97524219591, 0.971997845988, 0.979536887453]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.952620</td>\n",
       "      <td>[0.949407965554, 0.950996230479, 0.95745826602]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.961953</td>\n",
       "      <td>[0.95748116254, 0.961227786753, 0.967151319332]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.939340</td>\n",
       "      <td>[0.939720129171, 0.942380183091, 0.93591814755]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>[0.952637244349, 0.949919224556, 0.946688206785]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.923726</td>\n",
       "      <td>[0.921959095802, 0.926225094238, 0.922994076467]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.958184</td>\n",
       "      <td>[0.954790096878, 0.95745826602, 0.962304792676]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.911163</td>\n",
       "      <td>[0.913347685684, 0.910070005385, 0.910070005385]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.940596</td>\n",
       "      <td>[0.941334768568, 0.943995691976, 0.936456650512]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>[0.958557588805, 0.9633817986, 0.964997307485]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.963927</td>\n",
       "      <td>[0.962325080732, 0.962304792676, 0.967151319332]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.950287</td>\n",
       "      <td>[0.953175457481, 0.949919224556, 0.947765212709]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.955133</td>\n",
       "      <td>[0.958557588805, 0.954765751212, 0.952073236403]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.956748</td>\n",
       "      <td>[0.952099031216, 0.958535271944, 0.959612277868]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.964106</td>\n",
       "      <td>[0.95963401507, 0.964997307485, 0.967689822294]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.947236</td>\n",
       "      <td>[0.949946178687, 0.948842218632, 0.942918686053]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.953159</td>\n",
       "      <td>[0.954251883746, 0.953688745288, 0.951534733441]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.968593</td>\n",
       "      <td>[0.967168998924, 0.96661281637, 0.971997845988]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.965901</td>\n",
       "      <td>[0.965016146394, 0.964458804523, 0.968228325256]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.962491</td>\n",
       "      <td>[0.967707212056, 0.961766289715, 0.957996768982]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.960337</td>\n",
       "      <td>[0.963401506997, 0.959073774906, 0.958535271944]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.969490</td>\n",
       "      <td>[0.967168998924, 0.970920840065, 0.970382337103]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': Non...</td>\n",
       "      <td>0.966260</td>\n",
       "      <td>[0.965554359526, 0.967151319332, 0.966074313409]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>True</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.962670</td>\n",
       "      <td>[0.966630785791, 0.962304792676, 0.959073774906]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ridge__alpha': 10, 'tf_idf__stop_words': 'en...</td>\n",
       "      <td>0.961235</td>\n",
       "      <td>[0.964477933262, 0.960689283791, 0.958535271944]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ridge__alpha  tf_idf__min_df tf_idf__ngram_range tf_idf__stop_words  \\\n",
       "0             0.1               1              (1, 1)               None   \n",
       "1             0.1               1              (1, 1)               None   \n",
       "2             0.1               1              (1, 1)            english   \n",
       "3             0.1               1              (1, 1)            english   \n",
       "4             0.1               1              (1, 2)               None   \n",
       "5             0.1               1              (1, 2)               None   \n",
       "6             0.1               1              (1, 2)            english   \n",
       "7             0.1               1              (1, 2)            english   \n",
       "8             0.1               3              (1, 1)               None   \n",
       "9             0.1               3              (1, 1)               None   \n",
       "10            0.1               3              (1, 1)            english   \n",
       "11            0.1               3              (1, 1)            english   \n",
       "12            0.1               3              (1, 2)               None   \n",
       "13            0.1               3              (1, 2)               None   \n",
       "14            0.1               3              (1, 2)            english   \n",
       "15            0.1               3              (1, 2)            english   \n",
       "16            0.1              10              (1, 1)               None   \n",
       "17            0.1              10              (1, 1)               None   \n",
       "18            0.1              10              (1, 1)            english   \n",
       "19            0.1              10              (1, 1)            english   \n",
       "20            0.1              10              (1, 2)               None   \n",
       "21            0.1              10              (1, 2)               None   \n",
       "22            0.1              10              (1, 2)            english   \n",
       "23            0.1              10              (1, 2)            english   \n",
       "24            0.3               1              (1, 1)               None   \n",
       "25            0.3               1              (1, 1)               None   \n",
       "26            0.3               1              (1, 1)            english   \n",
       "27            0.3               1              (1, 1)            english   \n",
       "28            0.3               1              (1, 2)               None   \n",
       "29            0.3               1              (1, 2)               None   \n",
       "..            ...             ...                 ...                ...   \n",
       "90            3.0              10              (1, 1)            english   \n",
       "91            3.0              10              (1, 1)            english   \n",
       "92            3.0              10              (1, 2)               None   \n",
       "93            3.0              10              (1, 2)               None   \n",
       "94            3.0              10              (1, 2)            english   \n",
       "95            3.0              10              (1, 2)            english   \n",
       "96           10.0               1              (1, 1)               None   \n",
       "97           10.0               1              (1, 1)               None   \n",
       "98           10.0               1              (1, 1)            english   \n",
       "99           10.0               1              (1, 1)            english   \n",
       "100          10.0               1              (1, 2)               None   \n",
       "101          10.0               1              (1, 2)               None   \n",
       "102          10.0               1              (1, 2)            english   \n",
       "103          10.0               1              (1, 2)            english   \n",
       "104          10.0               3              (1, 1)               None   \n",
       "105          10.0               3              (1, 1)               None   \n",
       "106          10.0               3              (1, 1)            english   \n",
       "107          10.0               3              (1, 1)            english   \n",
       "108          10.0               3              (1, 2)               None   \n",
       "109          10.0               3              (1, 2)               None   \n",
       "110          10.0               3              (1, 2)            english   \n",
       "111          10.0               3              (1, 2)            english   \n",
       "112          10.0              10              (1, 1)               None   \n",
       "113          10.0              10              (1, 1)               None   \n",
       "114          10.0              10              (1, 1)            english   \n",
       "115          10.0              10              (1, 1)            english   \n",
       "116          10.0              10              (1, 2)               None   \n",
       "117          10.0              10              (1, 2)               None   \n",
       "118          10.0              10              (1, 2)            english   \n",
       "119          10.0              10              (1, 2)            english   \n",
       "\n",
       "    tf_idf__use_idf                                         parameters  \\\n",
       "0              True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "1             False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "2              True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "3             False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "4              True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "5             False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "6              True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "7             False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "8              True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "9             False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "10             True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "11            False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "12             True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "13            False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "14             True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "15            False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "16             True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "17            False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "18             True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "19            False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "20             True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "21            False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': No...   \n",
       "22             True  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "23            False  {'ridge__alpha': 0.1, 'tf_idf__stop_words': 'e...   \n",
       "24             True  {'ridge__alpha': 0.3, 'tf_idf__stop_words': No...   \n",
       "25            False  {'ridge__alpha': 0.3, 'tf_idf__stop_words': No...   \n",
       "26             True  {'ridge__alpha': 0.3, 'tf_idf__stop_words': 'e...   \n",
       "27            False  {'ridge__alpha': 0.3, 'tf_idf__stop_words': 'e...   \n",
       "28             True  {'ridge__alpha': 0.3, 'tf_idf__stop_words': No...   \n",
       "29            False  {'ridge__alpha': 0.3, 'tf_idf__stop_words': No...   \n",
       "..              ...                                                ...   \n",
       "90             True  {'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...   \n",
       "91            False  {'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...   \n",
       "92             True  {'ridge__alpha': 3, 'tf_idf__stop_words': None...   \n",
       "93            False  {'ridge__alpha': 3, 'tf_idf__stop_words': None...   \n",
       "94             True  {'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...   \n",
       "95            False  {'ridge__alpha': 3, 'tf_idf__stop_words': 'eng...   \n",
       "96             True  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "97            False  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "98             True  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "99            False  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "100            True  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "101           False  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "102            True  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "103           False  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "104            True  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "105           False  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "106            True  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "107           False  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "108            True  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "109           False  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "110            True  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "111           False  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "112            True  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "113           False  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "114            True  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "115           False  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "116            True  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "117           False  {'ridge__alpha': 10, 'tf_idf__stop_words': Non...   \n",
       "118            True  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "119           False  {'ridge__alpha': 10, 'tf_idf__stop_words': 'en...   \n",
       "\n",
       "     mean_validation_score                              cv_validation_scores  \\\n",
       "0                 0.981874  [0.982238966631, 0.980613893376, 0.982767905223]   \n",
       "1                 0.982233  [0.983315392896, 0.981152396338, 0.982229402262]   \n",
       "2                 0.980617  [0.980624327234, 0.980075390415, 0.981152396338]   \n",
       "3                 0.981694  [0.980624327234, 0.981152396338, 0.983306408185]   \n",
       "4                 0.987437  [0.987621097955, 0.985460420032, 0.989229940765]   \n",
       "5                 0.986540  [0.985468245425, 0.985998922994, 0.988152934841]   \n",
       "6                 0.983668  [0.981162540366, 0.984921917071, 0.984921917071]   \n",
       "7                 0.983668  [0.983315392896, 0.983306408185, 0.984383414109]   \n",
       "8                 0.982412  [0.983853606028, 0.982229402262, 0.981152396338]   \n",
       "9                 0.982950  [0.983315392896, 0.983306408185, 0.982229402262]   \n",
       "10                0.977925  [0.981700753498, 0.975767366721, 0.976305869682]   \n",
       "11                0.978643  [0.981700753498, 0.976844372644, 0.977382875606]   \n",
       "12                0.985463   [0.98439181916, 0.985998922994, 0.985998922994]   \n",
       "13                0.985463  [0.984930032293, 0.984921917071, 0.986537425956]   \n",
       "14                0.980079  [0.981162540366, 0.978998384491, 0.980075390415]   \n",
       "15                0.980797  [0.982777179763, 0.980613893376, 0.978998384491]   \n",
       "16                0.980976    [0.982777179763, 0.978459881529, 0.9816908993]   \n",
       "17                0.980438  [0.981700753498, 0.978998384491, 0.980613893376]   \n",
       "18                0.976849  [0.977395048439, 0.976305869682, 0.976844372644]   \n",
       "19                0.977028  [0.977933261572, 0.975228863759, 0.977921378568]   \n",
       "20                0.982053  [0.983315392896, 0.982229402262, 0.980613893376]   \n",
       "21                0.981515    [0.983853606028, 0.9816908993, 0.978998384491]   \n",
       "22                0.978284  [0.980624327234, 0.977382875606, 0.976844372644]   \n",
       "23                0.978464  [0.979009687836, 0.977382875606, 0.978998384491]   \n",
       "24                0.983309     [0.98439181916, 0.9816908993, 0.983844911147]   \n",
       "25                0.982950   [0.98439181916, 0.981152396338, 0.983306408185]   \n",
       "26                0.982592  [0.983315392896, 0.981152396338, 0.983306408185]   \n",
       "27                0.982053    [0.983853606028, 0.980613893376, 0.9816908993]   \n",
       "28                0.987258  [0.987621097955, 0.984921917071, 0.989229940765]   \n",
       "29                0.986181  [0.984930032293, 0.985460420032, 0.988152934841]   \n",
       "..                     ...                                               ...   \n",
       "90                0.975951  [0.976318622174, 0.973074851912, 0.978459881529]   \n",
       "91                0.975592   [0.975780409042, 0.97253634895, 0.978459881529]   \n",
       "92                0.980438  [0.981700753498, 0.980075390415, 0.979536887453]   \n",
       "93                0.978643  [0.978471474704, 0.978998384491, 0.978459881529]   \n",
       "94                0.977028  [0.976318622174, 0.974690360797, 0.980075390415]   \n",
       "95                0.975592   [0.97524219591, 0.971997845988, 0.979536887453]   \n",
       "96                0.952620   [0.949407965554, 0.950996230479, 0.95745826602]   \n",
       "97                0.961953   [0.95748116254, 0.961227786753, 0.967151319332]   \n",
       "98                0.939340   [0.939720129171, 0.942380183091, 0.93591814755]   \n",
       "99                0.949749  [0.952637244349, 0.949919224556, 0.946688206785]   \n",
       "100               0.923726  [0.921959095802, 0.926225094238, 0.922994076467]   \n",
       "101               0.958184   [0.954790096878, 0.95745826602, 0.962304792676]   \n",
       "102               0.911163  [0.913347685684, 0.910070005385, 0.910070005385]   \n",
       "103               0.940596  [0.941334768568, 0.943995691976, 0.936456650512]   \n",
       "104               0.962312    [0.958557588805, 0.9633817986, 0.964997307485]   \n",
       "105               0.963927  [0.962325080732, 0.962304792676, 0.967151319332]   \n",
       "106               0.950287  [0.953175457481, 0.949919224556, 0.947765212709]   \n",
       "107               0.955133  [0.958557588805, 0.954765751212, 0.952073236403]   \n",
       "108               0.956748  [0.952099031216, 0.958535271944, 0.959612277868]   \n",
       "109               0.964106   [0.95963401507, 0.964997307485, 0.967689822294]   \n",
       "110               0.947236  [0.949946178687, 0.948842218632, 0.942918686053]   \n",
       "111               0.953159  [0.954251883746, 0.953688745288, 0.951534733441]   \n",
       "112               0.968593   [0.967168998924, 0.96661281637, 0.971997845988]   \n",
       "113               0.965901  [0.965016146394, 0.964458804523, 0.968228325256]   \n",
       "114               0.962491  [0.967707212056, 0.961766289715, 0.957996768982]   \n",
       "115               0.960337  [0.963401506997, 0.959073774906, 0.958535271944]   \n",
       "116               0.969490  [0.967168998924, 0.970920840065, 0.970382337103]   \n",
       "117               0.966260  [0.965554359526, 0.967151319332, 0.966074313409]   \n",
       "118               0.962670  [0.966630785791, 0.962304792676, 0.959073774906]   \n",
       "119               0.961235  [0.964477933262, 0.960689283791, 0.958535271944]   \n",
       "\n",
       "     rmse  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  \n",
       "5     NaN  \n",
       "6     NaN  \n",
       "7     NaN  \n",
       "8     NaN  \n",
       "9     NaN  \n",
       "10    NaN  \n",
       "11    NaN  \n",
       "12    NaN  \n",
       "13    NaN  \n",
       "14    NaN  \n",
       "15    NaN  \n",
       "16    NaN  \n",
       "17    NaN  \n",
       "18    NaN  \n",
       "19    NaN  \n",
       "20    NaN  \n",
       "21    NaN  \n",
       "22    NaN  \n",
       "23    NaN  \n",
       "24    NaN  \n",
       "25    NaN  \n",
       "26    NaN  \n",
       "27    NaN  \n",
       "28    NaN  \n",
       "29    NaN  \n",
       "..    ...  \n",
       "90    NaN  \n",
       "91    NaN  \n",
       "92    NaN  \n",
       "93    NaN  \n",
       "94    NaN  \n",
       "95    NaN  \n",
       "96    NaN  \n",
       "97    NaN  \n",
       "98    NaN  \n",
       "99    NaN  \n",
       "100   NaN  \n",
       "101   NaN  \n",
       "102   NaN  \n",
       "103   NaN  \n",
       "104   NaN  \n",
       "105   NaN  \n",
       "106   NaN  \n",
       "107   NaN  \n",
       "108   NaN  \n",
       "109   NaN  \n",
       "110   NaN  \n",
       "111   NaN  \n",
       "112   NaN  \n",
       "113   NaN  \n",
       "114   NaN  \n",
       "115   NaN  \n",
       "116   NaN  \n",
       "117   NaN  \n",
       "118   NaN  \n",
       "119   NaN  \n",
       "\n",
       "[120 rows x 9 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the regularization parameter alpha. Remember alpha is the inverse of C - so the smaller alpha the stronger the regularization will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge__alpha</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.981545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.982434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.981485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.977267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.954467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean\n",
       "ridge__alpha          \n",
       "0.1           0.981545\n",
       "0.3           0.982434\n",
       "1.0           0.981485\n",
       "3.0           0.977267\n",
       "10.0          0.954467"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby([\"ridge__alpha\"])[\"mean_validation_score\"].aggregate([np.mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the best results are when alpha is small, around  0.1 - 0.3. If we make alpha too large we get a significant decrease in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case I tuned where the model should use idf or only tf. The best model does use idf but let's see if we look across all the tuning settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tf_idf__use_idf</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>0.976038</td>\n",
       "      <td>0.009895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.974841</td>\n",
       "      <td>0.014802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean       std\n",
       "tf_idf__use_idf                    \n",
       "False            0.976038  0.009895\n",
       "True             0.974841  0.014802"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby([\"tf_idf__use_idf\"])[\"mean_validation_score\"].aggregate([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hmm interetsingly not using idf performs slightly better over the entire grid space we tried out. This might be because the sms messages aren't very long. Here's a quote from the sklearn documentation on tf-idf: While the tfâidf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. Very short texts are likely to have noisy tfâidf values while the binary occurrence info is more stable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
